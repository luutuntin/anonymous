as for the diamond situation of DAG graph, one solution is to place the grandfather node under one of the son node.

we take deep first to raad in the amr format and construct the graph data structure. Maybe there has been some project on github to read amr and visualize.

if we want to extract knowledge graph, the first thing is determining the concept and relation. the relation in amr is lexmmatized as verb and some internal relation like ":location" ":time" ":poss" (maybe we should conclude them); we should recognize them as modifier rather than relation. The relation should be pred.(like threaten, consider)

*who invented amr? Why we need amr rather than something else? In fact I think use the attribute rather than pred as the relation is more effecient.

in order to organize the results into the format of json, we need following infomation:
id; type; labels; descriptions; aliases; claims; sitelinks; lastrevid;

what is knowledge?

* the name entity's all attributes is knowledge.
* if a entity has a bunch of descriptions, we can establish a graph of the entity, then the node with a high degree is the knowledge.
* how to restore the knowledge from amr.
- 比如从标签可以直接得到知识的关系方面，比如从标签的两个节点上可以直接得到concept的值
- 比如从modifier的分类上可以直接得到
* 但是我们需要从节点中区分concept和relation


 但是这组知识独立出来是没有任何意义的，因为我们并不知道这个treaty到底是什么，以及其对应的arm sales到底是什么，这个就需要我们就行指代消解。即the treaty的the指的到底是什么treaty。对于指称的上下文关联，目前想到的办法只是前后句之间确定对象，比如后面的代词一定是前面出现过的句子中的某个对象。也许这个我们可以通过bidirectional LSTM来实现信息流动，确定指代的到底是什么。所以我们的处理目标是一整篇文章，在这篇文章中，我们对于每一个出现的实体（不一定是命名实体）进行注册，之后对于后面的每一个对象进行前面的关联和替代。比如，我们可以对后续生成的amr进行改进，用我们的注册编号来代替本来这个实体的原有编号。

 就是，在一篇新闻中，可能某一个命名实体会关联一个对象，比如build一个center，但是没有介绍这个center的具体名称，但是后续的介绍展开都是对于这个center进行的，这样的话我们就需要对这个center进行一个自我命名的过程，并对后续出现的center在注册的表中进行查找。

 现在有两种想法，一种是我们呢能不能再知识图谱的抽取中更加上下文一点，这意味这我们的知识图谱的抽取分类器具有长短期记忆性，而不仅仅是普通的二分类，它应当具有识别以前出现过的实体的能力。另一种是我能不能改进camr来添加这个功能。

 也就是说在amr中非常麻烦的一点是它把所有的修饰词全部都变成了动词谓词，（几乎所有的），这样就导致了你不知道什么应该作为中心词，对于知识图谱来说，我们首先要确定的是我们的句子需要围绕的是哪几个中心概念。
 这样的话我们甚至还需要联系每一个动词的framework看一看这个动词的Arg2在这个动词的语义框架中起到了什么角色，这样就非常麻烦了。我们更希望的而是能够通过比如说我们能非常准确的定位中心词，之后围绕这个中心词展开意义。我觉得对于一个知识来说，AMR的优点确实是在共指和消歧一块做的不错。

 2018/07/26 meeting:

 there are some tasks for me to resolve:

* we should categorize which pred is valuable for extract. For example the pred "announce" is not valuable but the preds "owns" "builds" "links" are valuable.
* maybe we can try to apply the priori rules to the extraction to the subgraph to decrease the amount of meaningful subgraphs. To realize this, I should:
** no components missed
** valuable pred
** learn how to transfer a pred-ARG structure to other structure like named-entity or modifiered entity or entity-attribute strututre.
** 


07/28/2018
* How we can decide what information is valuable and what is not. Or how do we design a feature to decide the "meaningness" of a pred? (if there is quant? named entity? role-arg-of? no modality? long-time-span(for example :domain :beneficiary :poss :consist-of .etc))
* The second step: transform the structure of amr to triple(this is easy because we don't need to do decorations at first, we just keep the original word seq and make the word list)
* The third step: How do we decorate the word so that the modifier can be a part of concept. (Because some modifier or determination appears in the form of verb)
* 


1. want/willing/estimate can not be recognized as a knowledge
2. state/say/announce can not be recognized as a knowledge
3. entity's modifier such as :location, :time can be recognized as a knowledge
4. quant is an important mark for knowledge;sometimes ':polarity' is a knowledge( there is no ...)
5. pred是and的，其所有op按照独立知识处理（？）

07/31/2018
* The amount of named_entity :location is very little, so our extraction center lies in extract the associated thing of the named_entity.
* firstly, we should locate the entity associated to the named_entity, and then we find attribute of the associated entity. 
** The question is: what speciality of AMr can help us improve the performance? Can we register the entity to form a dictionary?

08/01/2018
**today I have a talk with the professor, professor ask I to extract the knowledge like wiki. So the format is tuple.
* After tasks, maybe I can research neo4j, to extract knowledge about EVENT. The data model of event is more complicated so that it is easier to formalize the sent to event knowledge.
* at present I should focus on named entity.

I just found that there are only limited labels of automatic amr-transferring. So may be this can be a point to improve amr. But if there are other people who are doing the amr, so may be w



09/06/2018
I have figured out some more strong features:
Maybe I can complement the executer of one action
Maybe I should consider the predicate of one sentence and consider the time of it


09/13/18
* I should plug in the spaCy
* I should use the allenlp to process purified
* I should 


09/24/18
If I want to construct a complete project, I should

pass filename to bash script
bash script pass filename to python script
python get param and pass to function
function need to distinguish training and testing, the following are differences:
* amr to subgraph: same
* tuple: there is no last column for annnotation
* feature: there is no last column for annotation
* train / predict: not read the last column

09/27/18
The prediction is all wrong! The prediction can not even be stocastic. Only if I make the target completely random, the prediction can be random.
I think the reason is the feature. The first 6 features are not that expressive. And the last features could be encoded to more concisive.

* the predicate (if it is informative)
	which class(0 is misc)
* the labels (if it is informative)
* the named_entity num and position ()
* if it has some mark predicate
* feature importance rate, have different weights

Here are details of feature design:
* About the position of ne, use sigma(1 / level), because the more entity / the lower entity, the more possible knowledge
* The index of pred itself should be a feature because it will increment the simularities compared to true sample. If we just use 0/1, we will lose the advantage.
* About the label, compute simularity while match the pred and give a rank(not sure, need more considerations**don't required same pred).
** For the first-level label, they will get 1 * proportion of same label.
** For the second-level label, compared only same first-level label, they will get 2 * proportion of same label
** note that ARG0/1/2 should be told apart, because they have different semantic role.
* About the Sayterm/wish/temp, it is obviously an important feature, the feature shouldn't be coalesed into just pred feature. Because it has perculiar negative influences on predition and should be strengthened.
* About the clf, we should consider other clf like linear square method, decision-making tree besides svm
* Here we need to add a tech named 'ne shrink', because for large amr, one of his arg may consist of one thing modified by other ne.
* Unfortunately, if we take the rank method as above, we will lose the information from difference of labels. Some label like 'location' 'beneficiary' may be very informative. So we should encode all kinds of label (including misc) and calculate the num of them as feature?